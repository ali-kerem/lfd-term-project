{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BLG 454E - Learning from Data : Term Project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ali Kerem Bozkurt - 150190003\n",
    "#### Beyza Aydeniz - 150200039\n",
    "#### Elvan Teke - 150190102\n",
    "#### Hasan Fatih Durkaya - 150200074\n",
    "#### Ömer Yıldırım - 150190115"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import helpers\n",
    "import random as r\n",
    "from numpy import mean\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LassoLars\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "train_features = helpers.prepareFeatures(file_name=\"train_features.csv\", normalize=False, ohe_children=False, ohe_region=True)\n",
    "train_targets = helpers.prepareTargets(file_name=\"train_targets.csv\")\n",
    "\n",
    "test_features = helpers.prepareFeatures(file_name=\"test_features.csv\",normalize=False, ohe_children=False, ohe_region=True)\n",
    "\n",
    "# We could normalize the data but since there is not a high imbalance between features, it's not needed.\n",
    "# We could use one hot encoding for children but the models perform worse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "models = []\n",
    "avg_errors = []\n",
    "\n",
    "# Add models for CV\n",
    "\n",
    "models.append(LinearRegression())\n",
    "models.append(Ridge())\n",
    "models.append(LassoLars())\n",
    "models.append(KNeighborsRegressor())\n",
    "models.append(AdaBoostRegressor())\n",
    "models.append(XGBRegressor())\n",
    "models.append(GradientBoostingRegressor())\n",
    "models.append(RandomForestRegressor())\n",
    "models.append(XGBRFRegressor())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Pick seed\n",
    "r.seed(1)\n",
    "\n",
    "# Apply 5-fold CV on models\n",
    "for model in models:\n",
    "    errors = helpers.CV(features=train_features, targets=train_targets, model=model, n_splits=5)\n",
    "    avg_errors.append(mean(errors)) # Add average CV error for each model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model                            Average CV Error\n",
      "1. XGBRFRegressor                 : 23843083.950505532\n",
      "2. GradientBoostingRegressor      : 25198792.71095756\n",
      "3. RandomForestRegressor          : 27852270.473088093\n",
      "4. AdaBoostRegressor              : 28755060.24319027\n",
      "5. XGBRegressor                   : 33900657.37621619\n",
      "6. LinearRegression               : 39935390.85086703\n",
      "7. Ridge                          : 40195206.16401751\n",
      "8. LassoLars                      : 40331383.11951368\n",
      "9. KNeighborsRegressor            : 136466953.46121663\n"
     ]
    }
   ],
   "source": [
    "# Sort by average error in ascending order\n",
    "models = [model for _, model in sorted(zip(avg_errors, models))]\n",
    "avg_errors.sort()\n",
    "\n",
    "# Print errors\n",
    "print(\"   {0:32} Average CV Error\".format('Model'))\n",
    "for i in range(len(models)):\n",
    "    print(\"{0}. {1:30} : {2}\".format(i + 1, models[i].__class__.__name__, avg_errors[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Top regressors are XGBRFRegressor and GradientBoostingRegressor. We can combine them using VotingRegressor and parameter tune to get the best fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model CV error for each fold : [18326726.976872742, 29250880.46753822, 20351429.071030013, 28894706.821220055, 16663950.417996477]\n",
      "Average : 22697538.7509315\n"
     ]
    }
   ],
   "source": [
    "xgbrf = XGBRFRegressor(max_depth=4, n_estimators=200, subsample=0.4, colsample_bynode=1, min_child_weight=7)\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=70, learning_rate=0.2)\n",
    "\n",
    "final_model = VotingRegressor([('gbr', gbr), ('xgbrf', xgbrf)])\n",
    "\n",
    "final_cv_error = helpers.CV(features=train_features, targets=train_targets, model=final_model, n_splits=5)\n",
    "print(\"Final model CV error for each fold : {}\".format(final_cv_error))\n",
    "print(\"Average : {}\".format(mean(final_cv_error)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After parameter tuning with CV, we use the whole dataset to make our final submission."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model training error : 18476624.557480335\n"
     ]
    }
   ],
   "source": [
    "final_model.fit(train_features, train_targets)\n",
    "\n",
    "# Print final training error\n",
    "final_training_error = mse(train_targets, final_model.predict(train_features))\n",
    "print(\"Final model training error : {}\".format(final_training_error))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "helpers.createSubmission(model=final_model, test_features=test_features, submission_file=\"submission.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}