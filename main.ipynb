{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Learning from Data Term Project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ali Kerem Bozkurt - 150190003\n",
    "#### Beyza Aydeniz - 150200039\n",
    "#### Elvan Teke - 150190102\n",
    "#### Hasan Fatih Durkaya - 150200074\n",
    "#### Ömer Yıldırım - 150190115"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import helpers\n",
    "import random as r\n",
    "from numpy import mean\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LassoLars\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "train_features = helpers.prepareFeatures(\"train_features.csv\", normalize=False, ohe_children=False, ohe_region=True)\n",
    "train_targets = helpers.prepareTargets(\"train_targets.csv\")\n",
    "\n",
    "test_features = helpers.prepareFeatures(\"test_features.csv\",normalize=False, ohe_children=False, ohe_region=True)\n",
    "\n",
    "# We could normalize the data but since there is not a high imbalance between features, it's not needed.\n",
    "# We could use one hot encoding for children but the models perform worse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "models = []\n",
    "avg_errors = []\n",
    "\n",
    "# Add models for CV\n",
    "\n",
    "models.append(LinearRegression())\n",
    "models.append(Ridge())\n",
    "models.append(LassoLars())\n",
    "models.append(KNeighborsRegressor())\n",
    "models.append(AdaBoostRegressor())\n",
    "models.append(XGBRegressor())\n",
    "models.append(GradientBoostingRegressor())\n",
    "models.append(RandomForestRegressor())\n",
    "models.append(XGBRFRegressor())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Pick seed\n",
    "r.seed(1)\n",
    "\n",
    "# Apply 5-fold CV on models\n",
    "for model in models:\n",
    "    errors = helpers.CV(features=train_features, targets=train_targets, model=model, n_splits=5)\n",
    "    avg_errors.append(mean(errors)) # Add average CV error for each model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model                            Average CV Error\n",
      "1. XGBRFRegressor                 : 24057694.91484101\n",
      "2. GradientBoostingRegressor      : 24262940.30005169\n",
      "3. RandomForestRegressor          : 25662805.01407318\n",
      "4. AdaBoostRegressor              : 29981675.663254507\n",
      "5. XGBRegressor                   : 32468007.509918343\n",
      "6. LinearRegression               : 39857699.46223384\n",
      "7. Ridge                          : 39993464.69700228\n",
      "8. LassoLars                      : 40249508.09507123\n",
      "9. KNeighborsRegressor            : 144493572.30999446\n"
     ]
    }
   ],
   "source": [
    "# Sort by average error in ascending order\n",
    "models = [model for _, model in sorted(zip(avg_errors, models))]\n",
    "avg_errors.sort()\n",
    "\n",
    "# Print errors\n",
    "print(\"   {0:32} Average CV Error\".format('Model'))\n",
    "for i in range(len(models)):\n",
    "    print(\"{0}. {1:30} : {2}\".format(i + 1, models[i].__class__.__name__, avg_errors[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Top regressors are XGBRFRegressor and GradientBoostingRegressor. We can combine them using VotingRegressor and parameter tune to get the best fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model CV error for each fold : [15295592.045305327, 28410718.241410345, 22831759.522029623, 20755001.89100978, 28469308.505679592]\n",
      "Average : 23152476.041086935\n"
     ]
    }
   ],
   "source": [
    "xgbrf = XGBRFRegressor(max_depth=4, n_estimators=200, subsample=0.442, colsample_bynode=1, min_child_weight=7)\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=68, learning_rate=0.228)\n",
    "\n",
    "final_model = VotingRegressor([('gbr', gbr), ('xgbrf', xgbrf)])\n",
    "\n",
    "final_cv_error = helpers.CV(features=train_features, targets=train_targets, model=final_model, n_splits=5)\n",
    "print(\"Final model CV error for each fold : {}\".format(final_cv_error))\n",
    "print(\"Average : {}\".format(mean(final_cv_error)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After parameter tuning with CV, we use the whole dataset to make our final submission."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model training error : 18403159.7504674\n"
     ]
    }
   ],
   "source": [
    "final_model.fit(train_features, train_targets)\n",
    "\n",
    "# Print final training error\n",
    "final_training_error = mse(train_targets, final_model.predict(train_features))\n",
    "print(\"Final model training error : {}\".format(final_training_error))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "helpers.createSubmission(model=final_model, test_features=test_features, submissionFile=\"submission.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}